{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "370/370 [==============================] - 155s 415ms/step - loss: 0.0103 - accuracy: 0.5556 - val_loss: 0.0261 - val_accuracy: 0.6366\n",
      "Epoch 2/20\n",
      "370/370 [==============================] - 153s 413ms/step - loss: 0.0083 - accuracy: 0.5840 - val_loss: 0.0072 - val_accuracy: 0.5909\n",
      "Epoch 3/20\n",
      "370/370 [==============================] - 153s 413ms/step - loss: 0.0081 - accuracy: 0.5923 - val_loss: 0.0074 - val_accuracy: 0.6282\n",
      "Epoch 4/20\n",
      "370/370 [==============================] - 146s 395ms/step - loss: 0.0079 - accuracy: 0.5916 - val_loss: 0.0074 - val_accuracy: 0.4792\n",
      "Epoch 5/20\n",
      "370/370 [==============================] - 148s 399ms/step - loss: 0.0077 - accuracy: 0.5969 - val_loss: 0.0068 - val_accuracy: 0.6173\n",
      "Epoch 6/20\n",
      "370/370 [==============================] - 153s 415ms/step - loss: 0.0077 - accuracy: 0.5996 - val_loss: 0.0072 - val_accuracy: 0.5580\n",
      "Epoch 7/20\n",
      "370/370 [==============================] - 147s 397ms/step - loss: 0.0076 - accuracy: 0.6017 - val_loss: 0.0070 - val_accuracy: 0.5957\n",
      "Epoch 8/20\n",
      "370/370 [==============================] - 154s 416ms/step - loss: 0.0073 - accuracy: 0.6049 - val_loss: 0.0068 - val_accuracy: 0.5958\n",
      "Epoch 9/20\n",
      "370/370 [==============================] - 153s 414ms/step - loss: 0.0073 - accuracy: 0.6011 - val_loss: 0.0065 - val_accuracy: 0.6363\n",
      "Epoch 10/20\n",
      "370/370 [==============================] - 148s 400ms/step - loss: 0.0073 - accuracy: 0.6048 - val_loss: 0.0064 - val_accuracy: 0.6294\n",
      "Epoch 11/20\n",
      "370/370 [==============================] - 154s 417ms/step - loss: 0.0072 - accuracy: 0.6081 - val_loss: 0.0066 - val_accuracy: 0.6364\n",
      "Epoch 12/20\n",
      "370/370 [==============================] - 155s 419ms/step - loss: 0.0072 - accuracy: 0.6063 - val_loss: 0.0065 - val_accuracy: 0.6380\n",
      "Epoch 13/20\n",
      "370/370 [==============================] - 154s 417ms/step - loss: 0.0072 - accuracy: 0.6057 - val_loss: 0.0072 - val_accuracy: 0.6172\n",
      "Epoch 14/20\n",
      "370/370 [==============================] - 153s 414ms/step - loss: 0.0071 - accuracy: 0.6101 - val_loss: 0.0063 - val_accuracy: 0.6368\n",
      "Epoch 15/20\n",
      "370/370 [==============================] - 147s 398ms/step - loss: 0.0070 - accuracy: 0.6094 - val_loss: 0.0070 - val_accuracy: 0.6362\n",
      "Epoch 16/20\n",
      "370/370 [==============================] - 148s 399ms/step - loss: 0.0069 - accuracy: 0.6083 - val_loss: 0.0065 - val_accuracy: 0.6002\n",
      "Epoch 17/20\n",
      "370/370 [==============================] - 154s 416ms/step - loss: 0.0070 - accuracy: 0.6089 - val_loss: 0.0069 - val_accuracy: 0.5932\n",
      "Epoch 18/20\n",
      "370/370 [==============================] - 153s 414ms/step - loss: 0.0069 - accuracy: 0.6092 - val_loss: 0.0064 - val_accuracy: 0.6157\n",
      "Epoch 19/20\n",
      "370/370 [==============================] - 148s 399ms/step - loss: 0.0068 - accuracy: 0.6082 - val_loss: 0.0065 - val_accuracy: 0.6306\n",
      "Epoch 20/20\n",
      "370/370 [==============================] - 153s 415ms/step - loss: 0.0068 - accuracy: 0.6079 - val_loss: 0.0065 - val_accuracy: 0.5827\n",
      "Epoch 1/20\n",
      "370/370 [==============================] - 156s 421ms/step - loss: 0.0716 - accuracy: 0.5559 - val_loss: 0.1220 - val_accuracy: 0.6318\n",
      "Epoch 2/20\n",
      "370/370 [==============================] - 153s 415ms/step - loss: 0.0654 - accuracy: 0.5746 - val_loss: 0.0614 - val_accuracy: 0.6202\n",
      "Epoch 3/20\n",
      "370/370 [==============================] - 154s 417ms/step - loss: 0.0643 - accuracy: 0.5849 - val_loss: 0.0615 - val_accuracy: 0.5408\n",
      "Epoch 4/20\n",
      "370/370 [==============================] - 153s 415ms/step - loss: 0.0629 - accuracy: 0.5876 - val_loss: 0.0601 - val_accuracy: 0.5166\n",
      "Epoch 5/20\n",
      "370/370 [==============================] - 153s 414ms/step - loss: 0.0611 - accuracy: 0.5923 - val_loss: 0.0592 - val_accuracy: 0.5436\n",
      "Epoch 6/20\n",
      "370/370 [==============================] - 154s 415ms/step - loss: 0.0617 - accuracy: 0.5953 - val_loss: 0.0562 - val_accuracy: 0.5797\n",
      "Epoch 7/20\n",
      "370/370 [==============================] - 153s 414ms/step - loss: 0.0606 - accuracy: 0.5903 - val_loss: 0.0580 - val_accuracy: 0.6223\n",
      "Epoch 8/20\n",
      "370/370 [==============================] - 154s 416ms/step - loss: 0.0601 - accuracy: 0.5986 - val_loss: 0.0547 - val_accuracy: 0.6013\n",
      "Epoch 9/20\n",
      "370/370 [==============================] - 153s 414ms/step - loss: 0.0601 - accuracy: 0.6033 - val_loss: 0.0550 - val_accuracy: 0.6136\n",
      "Epoch 10/20\n",
      "370/370 [==============================] - 147s 398ms/step - loss: 0.0595 - accuracy: 0.5995 - val_loss: 0.0576 - val_accuracy: 0.6347\n",
      "Epoch 11/20\n",
      "370/370 [==============================] - 153s 415ms/step - loss: 0.0583 - accuracy: 0.6057 - val_loss: 0.0540 - val_accuracy: 0.5779\n",
      "Epoch 12/20\n",
      "370/370 [==============================] - 153s 415ms/step - loss: 0.0591 - accuracy: 0.6050 - val_loss: 0.0574 - val_accuracy: 0.6143\n",
      "Epoch 13/20\n",
      "370/370 [==============================] - 153s 415ms/step - loss: 0.0580 - accuracy: 0.6073 - val_loss: 0.0541 - val_accuracy: 0.5951\n",
      "Epoch 14/20\n",
      "370/370 [==============================] - 154s 415ms/step - loss: 0.0583 - accuracy: 0.6036 - val_loss: 0.0611 - val_accuracy: 0.5974\n",
      "Epoch 15/20\n",
      "370/370 [==============================] - 154s 415ms/step - loss: 0.0584 - accuracy: 0.6079 - val_loss: 0.0537 - val_accuracy: 0.6322\n",
      "Epoch 16/20\n",
      "370/370 [==============================] - 154s 415ms/step - loss: 0.0573 - accuracy: 0.6043 - val_loss: 0.0523 - val_accuracy: 0.6225\n",
      "Epoch 17/20\n",
      "370/370 [==============================] - 153s 414ms/step - loss: 0.0573 - accuracy: 0.6067 - val_loss: 0.0524 - val_accuracy: 0.6156\n",
      "Epoch 18/20\n",
      "370/370 [==============================] - 148s 399ms/step - loss: 0.0569 - accuracy: 0.6040 - val_loss: 0.0542 - val_accuracy: 0.5860\n",
      "Epoch 19/20\n",
      "370/370 [==============================] - 147s 397ms/step - loss: 0.0569 - accuracy: 0.6059 - val_loss: 0.0531 - val_accuracy: 0.5916\n",
      "Epoch 20/20\n",
      "370/370 [==============================] - 147s 398ms/step - loss: 0.0559 - accuracy: 0.6056 - val_loss: 0.0549 - val_accuracy: 0.6385\n",
      "Epoch 1/20\n",
      "370/370 [==============================] - 154s 416ms/step - loss: 0.5421 - accuracy: 0.5517 - val_loss: 0.6023 - val_accuracy: 0.6352\n",
      "Epoch 2/20\n",
      "370/370 [==============================] - 153s 414ms/step - loss: 0.5369 - accuracy: 0.5778 - val_loss: 0.5412 - val_accuracy: 0.5994\n",
      "Epoch 3/20\n",
      "370/370 [==============================] - 154s 416ms/step - loss: 0.5364 - accuracy: 0.5823 - val_loss: 0.5382 - val_accuracy: 0.6203\n",
      "Epoch 4/20\n",
      "370/370 [==============================] - 159s 430ms/step - loss: 0.5358 - accuracy: 0.5882 - val_loss: 0.5376 - val_accuracy: 0.6330\n",
      "Epoch 5/20\n",
      "370/370 [==============================] - 165s 445ms/step - loss: 0.5355 - accuracy: 0.5958 - val_loss: 0.5370 - val_accuracy: 0.6257\n",
      "Epoch 6/20\n",
      "370/370 [==============================] - 160s 434ms/step - loss: 0.5351 - accuracy: 0.5965 - val_loss: 0.5365 - val_accuracy: 0.6332\n",
      "Epoch 7/20\n",
      "370/370 [==============================] - 161s 436ms/step - loss: 0.5352 - accuracy: 0.6000 - val_loss: 0.5365 - val_accuracy: 0.5995\n",
      "Epoch 8/20\n",
      "370/370 [==============================] - 161s 435ms/step - loss: 0.5347 - accuracy: 0.5976 - val_loss: 0.5363 - val_accuracy: 0.6321\n",
      "Epoch 9/20\n",
      "370/370 [==============================] - 155s 418ms/step - loss: 0.5344 - accuracy: 0.6018 - val_loss: 0.5364 - val_accuracy: 0.6341\n",
      "Epoch 10/20\n",
      "370/370 [==============================] - 149s 404ms/step - loss: 0.5342 - accuracy: 0.6008 - val_loss: 0.5358 - val_accuracy: 0.6317\n",
      "Epoch 11/20\n",
      "370/370 [==============================] - 149s 404ms/step - loss: 0.5348 - accuracy: 0.6036 - val_loss: 0.5351 - val_accuracy: 0.6243\n",
      "Epoch 12/20\n",
      "370/370 [==============================] - 152s 410ms/step - loss: 0.5336 - accuracy: 0.6033 - val_loss: 0.5351 - val_accuracy: 0.6030\n",
      "Epoch 13/20\n",
      "370/370 [==============================] - 167s 451ms/step - loss: 0.5339 - accuracy: 0.6025 - val_loss: 0.5350 - val_accuracy: 0.6359\n",
      "Epoch 14/20\n",
      "370/370 [==============================] - 173s 468ms/step - loss: 0.5335 - accuracy: 0.6040 - val_loss: 0.5367 - val_accuracy: 0.6360\n",
      "Epoch 15/20\n",
      "370/370 [==============================] - 163s 441ms/step - loss: 0.5336 - accuracy: 0.6055 - val_loss: 0.5346 - val_accuracy: 0.5978\n",
      "Epoch 16/20\n",
      "370/370 [==============================] - 164s 443ms/step - loss: 0.5329 - accuracy: 0.6033 - val_loss: 0.5348 - val_accuracy: 0.6147\n",
      "Epoch 17/20\n",
      "370/370 [==============================] - 166s 447ms/step - loss: 0.5330 - accuracy: 0.6043 - val_loss: 0.5360 - val_accuracy: 0.6315\n",
      "Epoch 18/20\n",
      "370/370 [==============================] - 166s 450ms/step - loss: 0.5331 - accuracy: 0.6057 - val_loss: 0.5405 - val_accuracy: 0.5580\n",
      "Epoch 19/20\n",
      "370/370 [==============================] - 167s 453ms/step - loss: 0.5332 - accuracy: 0.6007 - val_loss: 0.5348 - val_accuracy: 0.5890\n",
      "Epoch 20/20\n",
      "370/370 [==============================] - 168s 454ms/step - loss: 0.5328 - accuracy: 0.6004 - val_loss: 0.5345 - val_accuracy: 0.6169\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Concatenate, BatchNormalization, Activation\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "import random\n",
    "\n",
    "data_dir = \"cv_p3_images_split\"\n",
    "train_gray_dir = os.path.join(data_dir, \"train/grayscale\")\n",
    "train_color_dir = os.path.join(data_dir, \"train/colored\")\n",
    "val_gray_dir = os.path.join(data_dir, \"validation/grayscale\")\n",
    "val_color_dir = os.path.join(data_dir, \"validation/colored\")\n",
    "\n",
    "IMG_HEIGHT, IMG_WIDTH = 256, 256\n",
    "\n",
    "def preprocess_image(image_path, target_size):\n",
    "    image = load_img(image_path, target_size=target_size, color_mode=\"rgb\")\n",
    "    image = img_to_array(image) / 255.0\n",
    "    return image\n",
    "\n",
    "def load_images_from_folder(folder, target_size):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        images.append(preprocess_image(img_path, target_size))\n",
    "    return np.array(images)\n",
    "\n",
    "def load_dataset(gray_folder, color_folder, target_size, fraction=1.0):\n",
    "    gray_images = []\n",
    "    color_images = []\n",
    "    \n",
    "    filenames = os.listdir(gray_folder)\n",
    "    if fraction < 1.0:\n",
    "        filenames = random.sample(filenames, int(len(filenames) * fraction))\n",
    "    \n",
    "    for filename in filenames:\n",
    "        gray_path = os.path.join(gray_folder, filename)\n",
    "        color_path = os.path.join(color_folder, filename)\n",
    "        gray_images.append(preprocess_image(gray_path, target_size)[..., 0:1])\n",
    "        color_images.append(preprocess_image(color_path, target_size))\n",
    "    \n",
    "    return np.array(gray_images), np.array(color_images)\n",
    "\n",
    "\n",
    "train_gray, train_color = load_dataset(train_gray_dir, train_color_dir, (IMG_HEIGHT, IMG_WIDTH), 0.25)\n",
    "val_gray, val_color = load_dataset(val_gray_dir, val_color_dir, (IMG_HEIGHT, IMG_WIDTH), 0.25)\n",
    "\n",
    "def build_model(loss=\"mse\"):\n",
    "    inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1))\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\", strides=2)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding=\"same\", strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(3, (3, 3), padding=\"same\")(x)\n",
    "    outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "model_mse = build_model()\n",
    "model_mse.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
    "history = model_mse.fit(\n",
    "    train_gray, train_color,\n",
    "    validation_data=(val_gray, val_color),\n",
    "    epochs=20,\n",
    "    batch_size=16\n",
    ")\n",
    "model_mse.save(\"colorization_model_mse.h5\")\n",
    "\n",
    "model_mae = build_model(loss=\"mae\")\n",
    "model_mae.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"accuracy\"])\n",
    "history = model_mae.fit(\n",
    "    train_gray, train_color,\n",
    "    validation_data=(val_gray, val_color),\n",
    "    epochs=20,\n",
    "    batch_size=16\n",
    ")\n",
    "model_mae.save(\"colorization_model_mae.h5\")\n",
    "\n",
    "model_bce = build_model(loss=\"binary_crossentropy\")\n",
    "model_bce.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model_bce.fit(\n",
    "    train_gray, train_color,\n",
    "    validation_data=(val_gray, val_color),\n",
    "    epochs=20,\n",
    "    batch_size=16\n",
    ")\n",
    "model_bce.save(\"colorization_model_bce.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean SSIM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(ssim_scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Compare the model's output\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mcompare_colorized_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_mae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_gray_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_color_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [5], line 32\u001b[0m, in \u001b[0;36mcompare_colorized_images\u001b[1;34m(model, gray_dir, color_dir)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Compute comparison metrics\u001b[39;00m\n\u001b[0;32m     31\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(ground_truth\u001b[38;5;241m.\u001b[39mflatten(), predicted_color\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m---> 32\u001b[0m ssim_value \u001b[38;5;241m=\u001b[39m \u001b[43mssim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_color\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultichannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\n\u001b[0;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m mse_scores\u001b[38;5;241m.\u001b[39mappend(mse)\n\u001b[0;32m     37\u001b[0m ssim_scores\u001b[38;5;241m.\u001b[39mappend(ssim_value)\n",
      "File \u001b[1;32mc:\\Users\\kubal\\anaconda3\\envs\\computerVision\\lib\\site-packages\\skimage\\metrics\\_structural_similarity.py:186\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[1;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         win_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m  \u001b[38;5;66;03m# backwards compatibility\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many((np\u001b[38;5;241m.\u001b[39masarray(im1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m win_size) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin_size exceeds image extent. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEither ensure that your images are \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least 7x7; or pass win_size explicitly \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min the function call, with an odd value \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless than or equal to the smaller side of your \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages. If your images are multichannel \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(with color channels), set channel_axis to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe axis number corresponding to the channels.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    195\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (win_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindow size must be odd.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Directories\n",
    "test_gray_dir = os.path.join(data_dir, \"test/grayscale\")\n",
    "test_color_dir = os.path.join(data_dir, \"test/colored\")  # Ground truth colored images\n",
    "\n",
    "# Comparison function\n",
    "def compare_colorized_images(model, gray_dir, color_dir):\n",
    "    mse_scores = []\n",
    "    ssim_scores = []\n",
    "\n",
    "    for filename in os.listdir(gray_dir):\n",
    "        # Load and preprocess grayscale image\n",
    "        gray_path = os.path.join(gray_dir, filename)\n",
    "        gray_image = preprocess_image(gray_path, (IMG_HEIGHT, IMG_WIDTH))[..., 0:1]\n",
    "        gray_image = np.expand_dims(gray_image, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Predict the colorized output\n",
    "        predicted_color = model.predict(gray_image)[0]\n",
    "        predicted_color = np.clip(predicted_color * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Load ground truth colored image\n",
    "        gt_path = os.path.join(color_dir, filename)\n",
    "        ground_truth = preprocess_image(gt_path, (IMG_HEIGHT, IMG_WIDTH))\n",
    "\n",
    "\n",
    "        # Compute comparison metrics\n",
    "        mse = mean_squared_error(ground_truth.flatten(), predicted_color.flatten())\n",
    "        ssim_value = ssim(\n",
    "            ground_truth,\n",
    "            predicted_color,\n",
    "            win_size=None,  # Automatically adjust window size\n",
    "            data_range=255,\n",
    "            channel_axis=-1  # Specify that the last axis is the color channel\n",
    "        )\n",
    "\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        ssim_scores.append(ssim_value)\n",
    "\n",
    "        print(f\"{filename}: MSE = {mse:.2f}, SSIM = {ssim_value:.3f}\")\n",
    "\n",
    "    # Print average metrics\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    print(f\"Mean MSE: {np.mean(mse_scores):.2f}\")\n",
    "    print(f\"Mean SSIM: {np.mean(ssim_scores):.3f}\")\n",
    "\n",
    "# Compare the model's output\n",
    "compare_colorized_images(model_mae, test_gray_dir, test_color_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
