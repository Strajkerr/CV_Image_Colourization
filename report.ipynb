{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Project 3 - Image Colourization on pets dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared by:\n",
    " - Marianna Myszkowska 156041\n",
    " - Jakub Liszyński 156060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set\n",
    "The Oxford-IIIT Pet Dataset is a comprehensive collection of images featuring 37 distinct pet breeds, including both cats and dogs, with approximately 200 images per breed. The dataset offers significant diversity in terms of scale, pose, and lighting conditions, making it valuable for various computer vision tasks.\n",
    "\n",
    "### Key Features:\n",
    "- **Breed Annotations:** Each image is labeled with its respective breed, facilitating classification tasks.\n",
    "- **Head Region of Interest (ROI):** Annotations include bounding boxes around the pet's head, aiding in localization studies.\n",
    "- **Pixel-Level Trimap Segmentation:** Detailed annotations provide pixel-wise segmentation, distinguishing between the pet, foreground, and background, which is particularly useful for segmentation tasks.\n",
    "\n",
    "## Sample Images:\n",
    "Here are a few examples from the dataset:\n",
    "\n",
    "\n",
    "![card](raport_sources\\dataset-card.png) \n",
    "\n",
    "\n",
    "In order to use this dataset for our project we wil first convert images to grayscale and only then we wil use them. The Original images will be used as a reference while coloring the images.\n",
    "\n",
    "![colored](raport_sources\\Abyssinian_1.jpg) \n",
    "![gray](raport_sources\\Abyssinian_1_gray.jpg) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "def convert_images_to_grayscale(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    if not os.listdir(input_folder):\n",
    "        raise ValueError('Input folder is empty. Please ensure it contains images.')\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(input_folder, filename)\n",
    "            img = Image.open(img_path).convert('L')\n",
    "            gray_img_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_gray{os.path.splitext(filename)[1]}\")\n",
    "            img.save(gray_img_path)\n",
    "            print(f\"Converted {filename} to grayscale and saved as {gray_img_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For more information and to access the dataset, please visit the [official website](https://www.robots.ox.ac.uk/~vgg/data/pets/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem - Image colorization\n",
    "Image colorization is the process of adding plausible color information to grayscale images or videos. This task is inherently challenging due to its ill-posed nature; a single grayscale image can correspond to multiple valid color interpretations. Consequently, the model must infer and predict realistic colors for each pixel, often relying on learned patterns and contextual cues. \n",
    "\n",
    "Traditionally, colorization was performed manually by artists, which was time-consuming and required significant expertise. With advancements in deep learning, automated approaches have been developed to tackle this problem. These methods typically involve training convolutional neural networks (CNNs) on large datasets of color images. During training, the models learn to map grayscale inputs to their corresponding color outputs, capturing semantic and contextual information to produce realistic colorizations.\n",
    "\n",
    "Despite these advancements, challenges remain. The ambiguity of the task means that multiple color outputs can be correct for a single grayscale input. For instance, a grayscale image of a car could be red, blue, or any other color, and all would be plausible. Addressing this uncertainty is a key focus in current research, with approaches exploring probabilistic models and user-guided colorization to refine results. \n",
    "\n",
    "In summary, image colorization is a complex and underdetermined problem that seeks to enrich grayscale images by predicting and applying appropriate colors, leveraging machine learning techniques to achieve this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used architectures\n",
    "\n",
    "## 1. Tensorflow approach\n",
    "\n",
    "The architecture used for the image colorization task is a Convolutional Neural Network (CNN) designed to map grayscale images to their corresponding color images. The model consists of several layers, each with a specific function:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - **Shape:** (256, 256, 1)\n",
    "   - **Description:** This layer accepts grayscale images of size 256x256 pixels with a single channel.\n",
    "\n",
    "2. **Convolutional Layer 1:**\n",
    "   - **Filters:** 64\n",
    "   - **Kernel Size:** (3, 3)\n",
    "   - **Activation:** ReLU\n",
    "   - **Padding:** Same\n",
    "   - **Description:** This layer applies 64 convolutional filters to the input image, each of size 3x3, and uses the ReLU activation function to introduce non-linearity. The 'same' padding ensures that the output has the same spatial dimensions as the input.\n",
    "\n",
    "3. **UpSampling Layer 1:**\n",
    "   - **Size:** (2, 2)\n",
    "   - **Description:** This layer upsamples the input by a factor of 2, effectively doubling the spatial dimensions of the feature maps.\n",
    "\n",
    "4. **Convolutional Layer 2:**\n",
    "   - **Filters:** 32\n",
    "   - **Kernel Size:** (3, 3)\n",
    "   - **Activation:** ReLU\n",
    "   - **Padding:** Same\n",
    "   - **Description:** Similar to the first convolutional layer, but with 32 filters.\n",
    "\n",
    "5. **UpSampling Layer 2:**\n",
    "   - **Size:** (2, 2)\n",
    "   - **Description:** This layer upsamples the input by a factor of 2 again.\n",
    "\n",
    "6. **Convolutional Layer 3:**\n",
    "   - **Filters:** 16\n",
    "   - **Kernel Size:** (3, 3)\n",
    "   - **Activation:** ReLU\n",
    "   - **Padding:** Same\n",
    "   - **Description:** Similar to the previous convolutional layers, but with 16 filters.\n",
    "\n",
    "7. **Output Convolutional Layer:**\n",
    "   - **Filters:** 3\n",
    "   - **Kernel Size:** (3, 3)\n",
    "   - **Activation:** Sigmoid\n",
    "   - **Padding:** Same\n",
    "   - **Description:** This layer produces the final output with 3 channels (representing the RGB color channels) and uses the sigmoid activation function to ensure the output values are between 0 and 1.\n",
    "\n",
    "### Model Diagram\n",
    "Below is a diagram representing the architecture of the model:\n",
    "\n",
    "```\n",
    "Input (256, 256, 1) \n",
    "    ↓\n",
    "Conv2D (64 filters, 3x3, ReLU, same padding) \n",
    "    ↓\n",
    "UpSampling2D (2x2) \n",
    "    ↓\n",
    "Conv2D (32 filters, 3x3, ReLU, same padding) \n",
    "    ↓\n",
    "UpSampling2D (2x2) \n",
    "    ↓\n",
    "Conv2D (16 filters, 3x3, ReLU, same padding) \n",
    "    ↓\n",
    "Conv2D (3 filters, 3x3, Sigmoid, same padding) \n",
    "    ↓\n",
    "Output (256, 256, 3) \n",
    "```\n",
    "![model](raport_sources\\tensorflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pytorch approach\n",
    "One approach involved designing a neural network with an encoder-decoder structure to predict colors from grayscale inputs. A dataset of paired grayscale and color images was prepared, and the model was tested briefly using mixed-precision training. However, this specific approach was abandoned after initial trials, due to extremly slow learning, even tho it was trained using GPU, thats why we never finished learning process and abandon this at very start.\n",
    "\n",
    "````PYTHON\n",
    "# Dataset class\n",
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # Input: 1x256x256\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # Downsample: 128x128x128\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # Downsample: 256x64x64\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # Bottleneck\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # Upsample: 256x128x128\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # Upsample: 128x256x256\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),  # Output: 3x256x256\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis\n",
    "model analysis: size in memory, number of parameters,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "description of the training and the required commands to run it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "description of used metrics, loss, and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "plots: training and validation loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "### Optimizers\n",
    "#### 1.Adam Optimizer:\n",
    "\n",
    "-Combines the benefits of Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSprop).\n",
    "-Adjusts the learning rate individually for each parameter, making it highly effective and widely used.\n",
    "-Suitable for models with sparse gradients or when fine-tuning, often leading to fast convergence.\n",
    "-Tested with a learning rate of 0.001, showing stable training behavior.\n",
    "![OPTIMIZER PLOT](raport_sources\\adam.png)\n",
    "\n",
    "#### 2.SGD (Stochastic Gradient Descent):\n",
    "\n",
    "-A classic optimizer that updates weights based on the gradient of the loss function.\n",
    "-Momentum was added (0.9) to help accelerate convergence and overcome local minima.\n",
    "-Generally slower compared to adaptive optimizers but can yield better generalization with proper tuning.\n",
    "-Learning rate set to 0.001; its simplicity makes it sensitive to such settings.\n",
    "![OPTIMIZER PLOT](raport_sources\\sgd.png)\n",
    "\n",
    "\n",
    "#### 3.RMSprop:\n",
    "\n",
    "-A popular adaptive learning rate optimizer, especially for recurrent neural networks (RNNs).\n",
    "-Divides the learning rate by an exponentially decaying average of squared gradients, which helps balance learning -across parameters.\n",
    "-Works well in scenarios with non-stationary objectives or noisy gradients.\n",
    "-Like Adam, it was tested with a learning rate of 0.001 and proved effective in smoothing the optimization process.\n",
    "![OPTIMIZER PLOT](raport_sources\\rms.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Each optimizer was tested over 3 epochs on the same model and dataset with reduced steps for quicker evaluation. Validation loss was tracked to compare performance. Results showed the nuances of each optimizer, highlighting their strengths and trade-offs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![OPTIMIZER PLOT](raport_sources\\OPTIMIZER.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "the original image:\n",
    "\n",
    "\n",
    "![OPTIMIZER PLOT](raport_sources\\Abyssinian_2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "comparison of models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "list of libraries and tools used can be a requirements.txt file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rutime enviroment\n",
    "a description of the runtime environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and inference time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography \n",
    "preparation of a bibliography - the bibliography should contain references to the data set (preferably the article in which the collection was presented) and all scientific works and studies, including websites with tips on the solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points\n",
    "| **Task**                                         | **Status**      | **Points** |\n",
    "|--------------------------------------------------|-----------------|------------|\n",
    "| **Problem: Colorization**                        |  In progress    | 1          |\n",
    "| **Model: Pre-trained model (different problem)** |  In progress    | 1          |\n",
    "| **Data Augmentation**                            |  In progress    | 1          |\n",
    "| **Cross-Validation**                             |  Not Attempted  | 1          |\n",
    "| **Testing Optimizers (at least 3)**              |  Not Attempted  | 1          |\n",
    "| **Testing Loss Functions (at least 3)**          |  Not Attempted  | 1          |\n",
    "| **Dataset Requirements (at least 1000 images)**  |  Completed      | 0 (default requirement) |\n",
    "| **Metrics (at least 2)**                         |  Not Attempted  | 0 (default requirement) |\n",
    "| **Report (descriptions, diagrams, etc.)**        |  In progress    | 0          |\n",
    "| **Visualization Tools (e.g., TensorBoard)**      |  Not Attempted  | 1          |\n",
    "\n",
    "### **Points Summary**\n",
    "- **Problem**: 1 \n",
    "- **Model**: 1 \n",
    "- **Additional Points (Training, Dataset, Tools)**: 5 \n",
    "- **Total Points**: **7**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Link to Git](https://github.com/Strajkerr/CV_Image_Colourization)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
